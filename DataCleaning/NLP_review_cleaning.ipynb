{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T11:53:15.882242Z",
     "start_time": "2025-03-19T11:53:15.873035Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import udf\n",
    "from nltk.stem import SnowballStemmer\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pyspark.sql.functions import trim, lower, col\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T11:55:24.270465Z",
     "start_time": "2025-03-19T11:55:22.875312Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('final_cleaned_df.csv',on_bad_lines='skip')",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T11:56:45.464010Z",
     "start_time": "2025-03-19T11:56:45.430163Z"
    }
   },
   "cell_type": "code",
   "source": "df.head(10).to_csv('snippet.csv')",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the final review csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bognarlili/Big_Data_IMDb/Big_Data_IMDb\n",
      "mkdir: movie_reviews: File exists\n",
      "mv: /Users/bognarlili/Desktop/final_reviews_data.csv: No such file or directory\n",
      "final_reviews_data.csv\n"
     ]
    }
   ],
   "source": [
    "%cd Big_Data_IMDb/\n",
    "\n",
    "# add the csv files here otherwise the code wont work, cannot push it to GitHub\n",
    "# sadly, too large\n",
    "!mkdir movie_reviews\n",
    "%pwd\n",
    "\n",
    "!mv /Users/bognarlili/Desktop/final_reviews_data.csv movie_reviews/\n",
    "\n",
    "!ls movie_reviews/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading CSV into Spark"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T19:57:34.545640Z",
     "start_time": "2025-03-14T19:57:23.463136Z"
    }
   },
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IMDb Reviews Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"movie_reviews/final_reviews_data.csv\", header=True, inferSchema=True)\n",
    "df.show(5)  \n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------+----+-----+------------------+--------------------+--------------------+------------+--------------------+\n",
      "|   tconst|       movie_title|numVotes|year|label|tomatometer_status|         review_type|               genre|review_label|              review|\n",
      "+---------+------------------+--------+----+-----+------------------+--------------------+--------------------+------------+--------------------+\n",
      "|tt0009369|            mickey|  1119.0|1918|false|              NULL|                NULL|                NULL|        NULL|['Mack Sennett ha...|\n",
      "|tt0010600|          the doll|  1898.0|1919| true|              NULL|                NULL|                NULL|        NULL|['\"The Doll\" is a...|\n",
      "|tt0011439| the mark of zorro|  2439.0|1920| true|             Fresh|['Rotten', 'Fresh...|Action & Adventur...|       Fresh|['It was such a s...|\n",
      "|tt0011607|the parson's widow|  1264.0|1920| true|              NULL|                NULL|                NULL|        NULL|['Prästänkan (lit...|\n",
      "|tt0011841|     way down east|  5376.0|1920| true|             Fresh|['Fresh', 'Fresh'...|     Classics, Drama|       Fresh|['', 'The movie i...|\n",
      "+---------+------------------+--------+----+-----+------------------+--------------------+--------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T19:57:34.777694Z",
     "start_time": "2025-03-14T19:57:34.623859Z"
    }
   },
   "cell_type": "code",
   "source": "df.select('review').first()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(review='[\\'Mack Sennett had a strong reputation for producing wild\\', \\'violent\\', \\'fast- paced slapstick that often got its laughs without even a superficial attempt to make sense. He got that reputation for the simple reason that it\\'s true. However\\', \\'it\\'s interesting to see how when Sennett knew he had on his hands a comedian whose laughs come from subtleties or reactions rather than fast antics\\', \\'he knows to slacken the pace. That was true with many of the brilliant Harry Langdon shorts he would produce later\\', \\'and it is true here in \"Mickey\" with Mabel Normand.Mabel is the star and it is she on which the movie turns. She steals every scene she appears in and has infinite screen magnetism\\', \\'with her attractive\\', \\'fascinating face\\', \\'constantly changing expression\\', \\'and childlike and uninhibited yet somehow ironic manner. The greatest moments of comedy come in little bits of performance\\', \\'as Mabel comes up with many ingenious ways to hide dust she has swept up\\', \\'or simply can\\'t resist eating cherries off a cake.That said\\', \\'there are not actually a lot of scenes of overt comedy in this film\\', \\'and sometimes when there is overt comedy it comes out as a digression or bit of broad slapstick that is well-executed but has a different feel -- the battle in the country store (which looks a lot like the one Arbuckle worked at in \"The Butcher Boy\") over Mable\\'s dog or the animal the scurries up her pantleg. It\\'s not actually an uproariously funny film\\', \\'but doesn\\'t usually try to be\\', \\'and it\\'s always pleasant.The plot is simple and of a kind that has spawned infinite variations. Mabel is a rough-hewn girl from a miner town who loves playing with animals and skinny dipping (from a very wide angle); she is sent to her rich aunt and becomes involved in a kind of love square through no fault of her own. It\\'s really as much melodrama as anything else\\', \\'but it comes off. There are plenty of twists\\', \\'especially as the end draws near\\', \\'involving who is rich and who is poor when; these remain able to keep the interest\\', \\'and make a kind of commentary too\\', \\'intentional or not\\', \\'on the true insignificance of wealth.This has been cited as the first feature-length comedy starring a single comedian rather than an ensemble cast\\', \\'but even so it feels fairly developed as a form\\', \\'with decent pacing and plot developing in two places at once. This is a simple story well told\\', \\'and really made by its star\\', \\'who is well showcased.\\', \\'Country tomboy Mabel Normand (as Mickey) is sent from rustic Feather River\\', \\'California to live on her snooty aunt\\'s Long Island\\', \\'New York estate. The free-spirited Ms. Normand enjoys cavorting with animals - and skinny-dipping for cameramen with very long lenses. Nobody is sure if Normand owns a gold mine or is penniless\\', \\'so she is treated as both an heir and a servant (at different times). Normand attracts debonair millionaire Wheeler Oakman (as Herbert Thornhill) and overly amorous Lew Cody (as Reggie Drake). After numerous hi-jinks\\', \\'\"Mickey\" ends up with a winning man and (we hope) lives happily ever after...That Mabel Normand did not survive the ravages of fame was a great loss...Normand was an excellent actress and comedienne. Unfortunately\\', \\'she did not leave behind enough material to fully appreciate her worth. This production reportedly took two years to complete\\', \\'with Normand\\'s personal problems contributing to the delays. Fortunately\\', \\'her \"partying\" lifestyle does not adversely affect Normand\\'s performance as \"Mickey\". This was one of Normand\\'s best feature-length films. The plot is rather ordinary\\', \\'but the star handles each situation well. Herein\\', \\'she is most memorable portraying a carefree servant girl\\', \\'sliding down the banister and ingeniously sweeping her aunt\\'s dusty mansion floors.******* Mickey (8/11/18) F. Richard Jones ~ Mabel Normand\\', \\'Wheeler Oakman\\', \\'Lew Cody\\', \\'George Nichols\\', \\'The adventures of a gold miner\\'s daughter\\', \\'Mickey stars Mabel Normand\\', \\'who was one of the biggest film stars of the teens and 20s. In a series of episodes that are loosely connected\\', \\'Normand plays a Cinderella-like character who goes to live with a relative (Laura La Varnie) but when it\\'s discovered the gold mine is a bust\\', \\'she is made a maid in the household. But she catches the eye of the old lady\\'s daughter (Minta Durfee) and is eventually sent back to the country just as the mine strikes it big. The suitor (Wheeler Oakman) follows her. The plot seems to stray here and there without much narrative thread. At one point\\', \\'a lecher (Lewis Cody) is chasing Mabel around a mansion\\', \\'and then we\\'re off to the horse races. But while the episodes are tacked together\\', \\'Mabel Normand holds the viewer\\'s attention throughout the 90 minutes.This film was co-produced by Normand and Mack Sennett. George Nichols\\', \\'Minnie Devereaux\\', \\'Tom Kennedy\\', \\'and Edgar Kennedy co-star. And yes Minta Durfee was famous for being the wife of Roscoe \"Fatty\" Arbuckle\\', \\'a famous co-star with Normand in many short films. And the music by Neil Moret (who died in 1943) is absolutely great. I hummed the songs for days and learned to play them on the piano.\\', \\'Normand\\'s reputation is that she was a superbly gifted and beloved comedienne whose career was thwarted by scandal and her own reckless behavior. Having just read Lefler\\'s \"Mabel Normand The Life and Career of a Hollywood Madcap\\'\\', \\'I was surprised to learn that \\'Mickey\\' - a HUGE box-office success - was in the public domain and available. This film was so popular it was in continual theatrical release for 4 years - an astonishing feat! - when the average film run was 2 weeks.So I watched it. On YouTube. Not the ideal venue\\', \\'of course\\', \\'but even so\\', \\'I fail to see why this film was so popular. Lefler explains the historical context for it\\'s appeal to audiences\\', \\'but still\\', \\'\\'Mickey\\' seems to me to be rather ordinary. And worse\\', \\'I fail to see that Normand was superbly gifted as a comedienne. The biggest laughs - and there were too few of these - were provided by the dog - a good foil for Normand.Because this film is historically important\\', \\'it deserves to be given a top-tier restoration. Maybe the multi-million dollar Hollywood actors and producers can be cajoled into donating some of their millions for this purpose. Preserving the films of the industry of which they are a part should be to them a cause worthy of their support\\', \\'one that is no less important - and definitely more achievable - than saving the planet.\\', \\'\\'Mickey\\' jumps quickly from one silly scene to another\\', \\'with the idea of being a comedy\\', \\'but having so few laughs it feels more like drama. When the mean carer at the beginning is nothing but dominant and restrictive towards her it evokes sympathies\\', \\'not laughs. With a complete lack of cleverness in the script then it falls solely on Mabel Normand\\'s lively facial expressions to achieve this\\', \\'but talented as she is\\', \\'it is a bridge too far. Furthermore\\', \\'her bright\\', \\'energetic\\', \\'cutesy performance doesn\\'t sell tomboy to me at all - it doesn\\'t sell to me that she has been raised by this insensitive man for most of her years - and it is either miscasting\\', \\'misdirection\\', \\'poor scripting or a combination of all three.\\']')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove entries with missing review data as these movies are not present in the IMDB database and therefore represent incomplete data points that would skew our analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T19:57:36.125721Z",
     "start_time": "2025-03-14T19:57:34.912382Z"
    }
   },
   "source": [
    "# check the number of the rows\n",
    "# remove entries with missing review data\n",
    "# drop data before 1930\n",
    "print(f\"original nr of rows: {df.count()}\")\n",
    "df = df.dropna(subset=[\"review\"])\n",
    "df = df.filter(df[\"year\"] >= 1930)\n",
    "print(f\"nr of rows after cleaning: {df.count()}\")\n",
    "df.show(5)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original nr of rows: 8061\n",
      "nr of rows after cleaning: 7914\n",
      "+---------+--------------------+--------+----+-----+------------------+-----------+-----+------------+--------------------+\n",
      "|   tconst|         movie_title|numVotes|year|label|tomatometer_status|review_type|genre|review_label|              review|\n",
      "+---------+--------------------+--------+----+-----+------------------+-----------+-----+------------+--------------------+\n",
      "|tt0016029|  the little colonel|  1646.0|1935| true|              NULL|       NULL| NULL|        NULL|['THE LITTLE COLO...|\n",
      "|tt0017961|           happiness|  1080.0|1935| true|              NULL|       NULL| NULL|        NULL|['I really love s...|\n",
      "|tt0020298|         queen kelly|  3226.0|1932| true|              NULL|       NULL| NULL|        NULL|['I'd imagine tha...|\n",
      "|tt0020768|           city girl|  3199.0|1930| true|              NULL|       NULL| NULL|        NULL|['Silent film may...|\n",
      "|tt0021309|the story of the fox|     NaN|1937| true|              NULL|       NULL| NULL|        NULL|['One of the firs...|\n",
      "+---------+--------------------+--------+----+-----+------------------+-----------+-----+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T19:59:24.251154Z",
     "start_time": "2025-03-14T19:59:23.192298Z"
    }
   },
   "cell_type": "code",
   "source": "df.select('review').collect()[1][0]  # where row_number is the 0-indexed row you want (e.g., 2 for third row)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\'I really love silent cinema of all types\\', \\'and some of my very favorite films are silent (Voyage to the Moon\\', \\'Battleship Potemkin\\', \\'The Passion of Joan of Arc\\', \\'Our Hospitality\\', \\'Sherlock Jr.\\', \\'Safety Last\\', \\'City Lights\\', \\'Modern Times). Battleship Potemkin (and Sergei Eisenstein in general) got me interested in Soviet silent film\\', \\'so I was excited to find this particular title\\', \\'Happiness\\', \\'on my local video shelf (although that is not a link\\', \\'this is available for purchase\\', \\'if not at Amazon\\', \\'check other sites.  I picked it up and put it in my VCR. I had expected something heavy and powerful like Potemkin (and many other films that I had read about)\\', \\'but it turned out not to be exactly what I expected. Happiness is\\', \\'in fact\\', \\'a Russian silent comedy. No\\', \\'it is quite unlike the silent comedies of Keaton\\', \\'Lloyd\\', \\'and Chaplin. It shares a few characteristics (especially Chaplin\\'s famed swift-kick-in-the-butt\\', \\'so prevalent in his pre-feature-length shorts)\\', \\'but it is a lot more socially conscious. What\\'s more\\', \\'it manages to be both socially conscious (and very much so) AND quite humorous (also very much so). The main character\\', \\'actually named Loser\\', \\'is just a great character. He is the archetypical lovable loser. He can\\'t do a thing right\\', \\'ends up messing things up completely in every situation. At one point he is asked to protect his farming community\\'s storage barn\\', \\'and when trying to chase a goat away from some crops\\', \\'he fails to notice a bunch of people literally steal the house!Perhaps the funniest scene in the film is the one where Loser chooses to give up his life (strange in comedy: there are two different scenes with jokes about suicide\\', \\'and both actually work) because thieves stole his horse\\', \\'the last bit of property he owned (though it was a particularly bad horse). He builds a coffin\\', \\'measuring it for himself by stretching out in it to see if it will be comfortable. There is a great set piece where Loser is smoothing the wood of his coffin on a workhorse\\', \\'and when he moves back and forth\\', \\'shaving the wood\\', \\'the workhorse moves rhythmically back and forth like a real horse (maybe you\\'d have to see it to understand). The local magistrate catches him doing this (i.e.\\', \\'planning suicide) and he speaks a very telling line: \"If the peasants start killing themselves\\', \\'where will we get crops?\" \"Killing yourself is illegal\\', \\'\" the magistrate informs him. He gets the local wing of the army\\', \\'in which the commons soldiers are no longer individuals\\', \\'but rather automatons with big\\', \\'puffy\\', \\'rubber masks (they very much resemble the school children in the film Pink Floyd\\'s the Wall). The captain of this troop\\', \\'when Loser says that he can no longer live\\', \\'says to his soldiers: \"Beat him half to death!\"The only problem with this film is that I believe some scenes are missing. The story is sort of difficult to follow at times. Otherwise\\', \\'it is a very good film\\', \\'unfortunately forgotten as so many old films are. 8/10\\', \\'A hapless loser (with the surname of Loser) undergoes misadventures with avaricious clergy\\', \\'a tired horse\\', \\'and a walking granary (among other things) on his road to collectivized happiness.Unnoticed on its release\\', \\'\"Happiness\" became well-known in the 1960s among film scholars. It was especially championed by Chris Marker who included some excerpts from \"Happiness\" in his 1992 documentary \"The Last Bolshevik\". I wish it had been noticed sooner and was better known today.Soviet film\\', \\'at least in the early years\\', \\'tends to be serious and quite political. Here it may be political\\', \\'but it is anything but serious. There are some humorous moments mixed with some unusual camera tricks (watching food fly into the old man\\'s mouth is a surreal experience).\\', \\'The efforts of a farmer in pre-revolution Russia to improve his lot are frustrated by a system designed to exploit all hard work. Soviet propaganda dressed up as madcap silent comedy (reminiscent at times of Buster Keaton) is a truly bizarre\\', \\'semi-surreal watch. Crammed with grotesque caricatures and situations that become a little overwhelming after a while.\\', \\'Happiness as labor\\', \\'sure enough. A farmer sent by his wife to find it\\', \\'finds instead a wholly absurd Tsarist Russia where a saint in name would sooner drown than let go a bundle of rubles to the bottom of a river. But does he find it\\', \\'you would think at long last\\', \\'when a more subtly absurd Revolution sweeps the countryside?What a strange\\', \\'uplifting joy to be able to see this silent Soviet comedy about a hapless schmuck caught in the wheels of a callous world. The filmmaker could have gone for a harmless Chaplin effect\\', \\'assuming for a moment Stalin\\'s censors stayed their hand\\', \\'benevolent fates setting up pratfalls all the while taking care of life. The schmuck triumphs because he\\'s pure inside\\', \\'or a natural performer. But Chaplin was not honest with us. He was a hard worker\\', \\'a staunch Marxist\\', \\'but hard work is never a value in his films; no\\', \\'he made his fortunes by selling people the Dream.The inspiration here is Buster Keaton\\', \\'the stone face mute in the face of unfathomable mishap. Dogged effort.Let\\'s unpack this a little. The wife is strong and resolute to the end\\', \\'a hard worker\\', \\'and it pays off for her. But this is not one of those amazingly farcical works dubbed socialist realist at the time and favored by Stalin\\', \\'celebrating the flipside of that Dream as robust heroes of labor triumph in the fields in the name of the people. These fields are barren\\', \\'dusty\\', \\'crops are nowhere. Members of the commune are not all of them content worker bees\\', \\'they are also cruel\\', \\'weak\\', \\'suspicious\\', \\'human. Filthy brigands stalk the perimeters\\', \\'the first true revolutionaries for Bakhunin let\\'s not forget. The workers attack them with watermelons\\', \\'the fruits of labor wasted on a squabble.But this is not simple criticism or direct confrontation with the regime\\', \\'which sure enough would have earned Medvedkin a swift execution. Our gain is that this man was forced to find ways to dream further than the state-sponsored Dream had imagination to apprehend him.So what does he do? He envisions images with enough weight and shadow so that we fill the shape. He sketches dreamlike edges only. Anxious air. The man whipping his wife as she plows the field\\', \\'otherwise they starve. A shaggy horse towing on its back a wooden cabin as it staggers to grab a meagre bite of hay. A grain storage room absconding on human feet. The man tasked to guard it mystified by the over-sized rifle he\\'s given to guard it with. The same man later hidden in a chest and discovered by authorities by his sicle protruding from the chest.Make no mistake\\', \\'the film is speaking about Soviet life in the fields. Its realism is the restless dream\\', \\'the artifice\\', \\'the world a size it doesn\\'t fit the people.Curiously enough\\', \\'it got past the censors but was mostly panned by the press. His next film didn\\'t\\', \\'a major loss for us. Soon after\\', \\'he was rebuked by Party sycophants for daring to speak in favor of Eisenstein\\'s Bezhin Meadow\\', \\'then in production and soon to be likewise axed and destroyed. By then\\', \\'the late 30\\'s\\', \\'it was all over for everyone\\', \\'the film trains permanently derailed. On the other side\\', \\'Buster Keaton had missed his own place in a revolution and was on his way to become a weary\\', \\'beaten old man like the protagonist here.See if you can find Chris Marker\\'s Last Bolshevik\\', \\'a documentary where Medvedkin in his old age reminisces on all this.\\']'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T19:54:59.751193Z",
     "start_time": "2025-03-14T19:54:59.737224Z"
    }
   },
   "source": [
    "# Import necessary modules\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"\n",
    "    A class to handle text preprocessing steps including:\n",
    "    - Tokenization (splitting text into words)\n",
    "    - Stopword removal (removing common words like \"the\", \"is\", etc.)\n",
    "    - Stemming (reducing words to their root form)\n",
    "    - Lemmatization (reducing words to their base form)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor that initializes the stemmer and lemmatizer objects.\n",
    "        - SnowballStemmer: Used for stemming words.\n",
    "        - WordNetLemmatizer: Used for lemmatizing words.\n",
    "        \"\"\"\n",
    "        self.stemmer = SnowballStemmer(language=\"english\")\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def tokenize(self, df, input_col, output_col):\n",
    "        \"\"\"\n",
    "        Tokenizes the specified column into words using RegexTokenizer.\n",
    "\n",
    "        \"\"\"\n",
    "        tokenizer = RegexTokenizer(\n",
    "            inputCol=input_col, \n",
    "            outputCol=output_col, \n",
    "            pattern=\"\\\\W+\",   # Split on non-word characters\n",
    "            toLowercase=True  # Convert text to lowercase\n",
    "        )\n",
    "        return tokenizer.transform(df)\n",
    "    \n",
    "    def remove_stopwords(self, df, input_col, output_col):\n",
    "        \"\"\"\n",
    "        Removes stopwords (e.g., \"the\", \"and\", \"is\") using StopWordsRemover.\n",
    "            output_col (str): Name of the new column with stopwords removed.\n",
    "        \"\"\"\n",
    "        remover = StopWordsRemover(\n",
    "            inputCol=input_col,\n",
    "            outputCol=output_col\n",
    "        )\n",
    "        return remover.transform(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    @udf(ArrayType(StringType()))\n",
    "    def stemming(words):\n",
    "        \"\"\"\n",
    "        Applies stemming using SnowballStemmer to reduce words to their root form.\n",
    "\n",
    "        \"\"\"\n",
    "        if words is not None:\n",
    "            stemmer = SnowballStemmer(language=\"english\")\n",
    "            return [stemmer.stem(word) for word in words]\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    @udf(ArrayType(StringType()))\n",
    "    def lemmatizing(words):\n",
    "        \"\"\"\n",
    "        Applies lemmatization using WordNetLemmatizer to reduce words to their base form.\n",
    "\n",
    "        \"\"\"\n",
    "        if words is not None:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            return [lemmatizer.lemmatize(word) for word in words]\n",
    "        return None\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T19:55:45.600083Z",
     "start_time": "2025-03-14T19:55:34.674179Z"
    }
   },
   "source": [
    "#processor instance\n",
    "processor = TextProcessor()\n",
    "\n",
    "# Initialize SparkSession \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IMDb Reviews Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load CSV file into a PySpark DataFrame\n",
    "df = spark.read.csv(\"movie_reviews/final_reviews_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "#cleaning pipeline in one call\n",
    "\n",
    "def run_cleaning(df):\n",
    "    \"\"\"\n",
    "    Executes all preprocessing steps:\n",
    "    1. Tokenization\n",
    "    2. Stopword removal\n",
    "    3. Stemming\n",
    "    4. Lemmatization\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Processed DataFrame with cleaned text data.\n",
    "    \"\"\"\n",
    "    processor = TextProcessor()\n",
    "    \n",
    "    # Execute steps in order\n",
    "    df = processor.tokenize(df, input_col=\"review\", output_col=\"review_words\")\n",
    "    df = processor.remove_stopwords(df, input_col=\"review_words\", output_col=\"review_clean\")\n",
    "    df = df.withColumn(\"review_stemmed\", processor.stemming(col(\"review_clean\")))\n",
    "    df = df.withColumn(\"review_lemmatized\", processor.lemmatizing(col(\"review_stemmed\")))\n",
    "\n",
    "    return df\n",
    "\n",
    "df_cleaned = run_cleaning(df)\n",
    "df_cleaned.show(truncate=False)\n"
   ],
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o268.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 70) (10.30.1.148 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPy4JJavaError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 41\u001B[39m\n\u001B[32m     38\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[32m     40\u001B[39m df_cleaned = run_cleaning(df)\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m \u001B[43mdf_cleaned\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtruncate\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\VE\\BD\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001B[39m, in \u001B[36mDataFrame.show\u001B[39m\u001B[34m(self, n, truncate, vertical)\u001B[39m\n\u001B[32m    887\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m = \u001B[32m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] = \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    888\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001B[39;00m\n\u001B[32m    889\u001B[39m \n\u001B[32m    890\u001B[39m \u001B[33;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    945\u001B[39m \u001B[33;03m    name | Bob\u001B[39;00m\n\u001B[32m    946\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m947\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_show_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\VE\\BD\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:978\u001B[39m, in \u001B[36mDataFrame._show_string\u001B[39m\u001B[34m(self, n, truncate, vertical)\u001B[39m\n\u001B[32m    969\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[32m    970\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[32m    971\u001B[39m         error_class=\u001B[33m\"\u001B[39m\u001B[33mNOT_BOOL\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    972\u001B[39m         message_parameters={\n\u001B[32m   (...)\u001B[39m\u001B[32m    975\u001B[39m         },\n\u001B[32m    976\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m978\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mint_truncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\VE\\BD\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1316\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1317\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1318\u001B[39m     args_command +\\\n\u001B[32m   1319\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1321\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1323\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1325\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\VE\\BD\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdeco\u001B[39m(*a: Any, **kw: Any) -> Any:\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    180\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    181\u001B[39m         converted = convert_exception(e.java_exception)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\VE\\BD\\Lib\\site-packages\\py4j\\protocol.py:326\u001B[39m, in \u001B[36mget_return_value\u001B[39m\u001B[34m(answer, gateway_client, target_id, name)\u001B[39m\n\u001B[32m    324\u001B[39m value = OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[32m2\u001B[39m:], gateway_client)\n\u001B[32m    325\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[32m1\u001B[39m] == REFERENCE_TYPE:\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[32m    327\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    328\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name), value)\n\u001B[32m    329\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    330\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[32m    331\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    332\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name, value))\n",
      "\u001B[31mPy4JJavaError\u001B[39m: An error occurred while calling o268.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 70) (10.30.1.148 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NLP_review_cleaning.ipynb has been saved.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"NLP_review_cleaning.ipynb\"\n",
    "\n",
    "# Open and close the file to save it\n",
    "with open(file_path, \"a\") as f:\n",
    "    pass\n",
    "\n",
    "print(f\"✅ {file_path} has been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
